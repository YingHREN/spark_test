== Physical Plan ==
AdaptiveSparkPlan isFinalPlan=false
+- TakeOrderedAndProject(limit=100, orderBy=[c_customer_id#82 ASC NULLS FIRST], output=[c_customer_id#82])
   +- BroadcastNestedLoopJoin BuildRight, Inner
      :- Project [c_customer_id#82]
      :  +- BroadcastHashJoin [ctr_customer_sk#1], [c_customer_sk#81], Inner, BuildRight, false
      :     :- Project [ctr_customer_sk#1]
      :     :  +- SortMergeJoin [ctr_store_sk#2], [ctr_store_sk#103], Inner, (ctr_total_return#3 > (avg(ctr_total_return) * 1.2)#106)
      :     :     :- Sort [ctr_store_sk#2 ASC NULLS FIRST], false, 0
      :     :     :  +- Exchange hashpartitioning(ctr_store_sk#2, 200), ENSURE_REQUIREMENTS, [plan_id=1062]
      :     :     :     +- Filter isnotnull(ctr_total_return#3)
      :     :     :        +- HashAggregate(keys=[sr_customer_sk#7, sr_store_sk#11], functions=[sum(sr_return_amt#15)])
      :     :     :           +- Exchange hashpartitioning(sr_customer_sk#7, sr_store_sk#11, 200), ENSURE_REQUIREMENTS, [plan_id=1045]
      :     :     :              +- HashAggregate(keys=[sr_customer_sk#7, sr_store_sk#11], functions=[partial_sum(sr_return_amt#15)])
      :     :     :                 +- Project [sr_customer_sk#7, sr_store_sk#11, sr_return_amt#15]
      :     :     :                    +- BroadcastHashJoin [sr_returned_date_sk#4], [d_date_sk#24], Inner, BuildRight, false
      :     :     :                       :- Filter ((isnotnull(sr_returned_date_sk#4) AND isnotnull(sr_store_sk#11)) AND isnotnull(sr_customer_sk#7))
      :     :     :                       :  +- FileScan csv spark_catalog.default.store_returns[sr_returned_date_sk#4,sr_customer_sk#7,sr_store_sk#11,sr_return_amt#15] Batched: false, DataFilters: [isnotnull(sr_returned_date_sk#4), isnotnull(sr_store_sk#11), isnotnull(sr_customer_sk#7)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/renyinghao/projects/spark_test/tpc-ds/data/store_returns.dat], PartitionFilters: [], PushedFilters: [IsNotNull(sr_returned_date_sk), IsNotNull(sr_store_sk), IsNotNull(sr_customer_sk)], ReadSchema: struct<sr_returned_date_sk:int,sr_customer_sk:int,sr_store_sk:int,sr_return_amt:float>
      :     :     :                       +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=1040]
      :     :     :                          +- Project [d_date_sk#24]
      :     :     :                             +- Filter ((isnotnull(d_year#30) AND (d_year#30 = 2001)) AND isnotnull(d_date_sk#24))
      :     :     :                                +- FileScan csv spark_catalog.default.date_dim[d_date_sk#24,d_year#30] Batched: false, DataFilters: [isnotnull(d_year#30), (d_year#30 = 2001), isnotnull(d_date_sk#24)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/renyinghao/projects/spark_test/tpc-ds/data/date_dim.dat], PartitionFilters: [], PushedFilters: [IsNotNull(d_year), EqualTo(d_year,2001), IsNotNull(d_date_sk)], ReadSchema: struct<d_date_sk:int,d_year:int>
      :     :     +- Sort [ctr_store_sk#103 ASC NULLS FIRST], false, 0
      :     :        +- Filter isnotnull((avg(ctr_total_return) * 1.2)#106)
      :     :           +- HashAggregate(keys=[ctr_store_sk#103], functions=[avg(ctr_total_return#104)])
      :     :              +- Exchange hashpartitioning(ctr_store_sk#103, 200), ENSURE_REQUIREMENTS, [plan_id=1057]
      :     :                 +- HashAggregate(keys=[ctr_store_sk#103], functions=[partial_avg(ctr_total_return#104)])
      :     :                    +- HashAggregate(keys=[sr_customer_sk#363, sr_store_sk#367], functions=[sum(sr_return_amt#371)])
      :     :                       +- Exchange hashpartitioning(sr_customer_sk#363, sr_store_sk#367, 200), ENSURE_REQUIREMENTS, [plan_id=1053]
      :     :                          +- HashAggregate(keys=[sr_customer_sk#363, sr_store_sk#367], functions=[partial_sum(sr_return_amt#371)])
      :     :                             +- Project [sr_customer_sk#363, sr_store_sk#367, sr_return_amt#371]
      :     :                                +- BroadcastHashJoin [sr_returned_date_sk#360], [d_date_sk#380], Inner, BuildRight, false
      :     :                                   :- Filter (isnotnull(sr_returned_date_sk#360) AND isnotnull(sr_store_sk#367))
      :     :                                   :  +- FileScan csv spark_catalog.default.store_returns[sr_returned_date_sk#360,sr_customer_sk#363,sr_store_sk#367,sr_return_amt#371] Batched: false, DataFilters: [isnotnull(sr_returned_date_sk#360), isnotnull(sr_store_sk#367)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/renyinghao/projects/spark_test/tpc-ds/data/store_returns.dat], PartitionFilters: [], PushedFilters: [IsNotNull(sr_returned_date_sk), IsNotNull(sr_store_sk)], ReadSchema: struct<sr_returned_date_sk:int,sr_customer_sk:int,sr_store_sk:int,sr_return_amt:float>
      :     :                                   +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=1048]
      :     :                                      +- Project [d_date_sk#380]
      :     :                                         +- Filter ((isnotnull(d_year#386) AND (d_year#386 = 2001)) AND isnotnull(d_date_sk#380))
      :     :                                            +- FileScan csv spark_catalog.default.date_dim[d_date_sk#380,d_year#386] Batched: false, DataFilters: [isnotnull(d_year#386), (d_year#386 = 2001), isnotnull(d_date_sk#380)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/renyinghao/projects/spark_test/tpc-ds/data/date_dim.dat], PartitionFilters: [], PushedFilters: [IsNotNull(d_year), EqualTo(d_year,2001), IsNotNull(d_date_sk)], ReadSchema: struct<d_date_sk:int,d_year:int>
      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=1068]
      :        +- Filter isnotnull(c_customer_sk#81)
      :           +- FileScan csv spark_catalog.default.customer[c_customer_sk#81,c_customer_id#82] Batched: false, DataFilters: [isnotnull(c_customer_sk#81)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/renyinghao/projects/spark_test/tpc-ds/data/customer.dat], PartitionFilters: [], PushedFilters: [IsNotNull(c_customer_sk)], ReadSchema: struct<c_customer_sk:int,c_customer_id:string>
      +- BroadcastExchange IdentityBroadcastMode, [plan_id=1072]
         +- Project
            +- Filter (isnotnull(s_state#76) AND (s_state#76 = TN))
               +- FileScan csv spark_catalog.default.store[s_state#76] Batched: false, DataFilters: [isnotnull(s_state#76), (s_state#76 = TN)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/renyinghao/projects/spark_test/tpc-ds/data/store.dat], PartitionFilters: [], PushedFilters: [IsNotNull(s_state), EqualTo(s_state,TN)], ReadSchema: struct<s_state:string>